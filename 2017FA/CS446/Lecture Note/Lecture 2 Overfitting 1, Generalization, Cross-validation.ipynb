{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbor\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "+ dist : $\\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$\n",
    "    + Euclidean: $||x_i - x_j||^2_2$\n",
    "    + Minkowski(norm)\n",
    "    + etc...\n",
    "+ K : # nearest neighbors\n",
    "\n",
    "### Pro\n",
    "+ Learning is \"free\"\n",
    "+ Works well in practice\n",
    "\n",
    "### Con\n",
    "+ Test cost is high\n",
    "    + Euclidean, k\n",
    "        + Computation $O(nkd)$\n",
    "        + Memory $O(nd)$\n",
    "    + Tricks\n",
    "        + k-d tree\n",
    "        + hashing\n",
    "        \n",
    "## Inductive Bias\n",
    "\n",
    "Nearby points have similar labels (spatial smoothness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "## Performance\n",
    "\n",
    "### Metric\n",
    "+ Risk $R(h, \\mathbb{P})$, the lower the better\n",
    "    + Error rate: $Error(h, D)= \\frac{1}{n} \\sum 1_{[y_i \\neq h(x_i)]}$\n",
    "        + Train data\n",
    "        + New Test (validation) data\n",
    "            * Typical Data\n",
    "                * $x_i, y_i \\sim^{iid} \\mathbb{P}$\n",
    "            * Adversial Data\n",
    "\n",
    "        + For population $Error(h, D)= \\mathbb{E}_{(x, y) \\sim \\mathbb{P}} [1_{[y_i \\neq h(x_i)]}] = \\mathbb{P}(y \\neq h(x))$\n",
    "        + Generalization Error: $G_n = Error(h_n, \\mathbb{P}) - Error(h_n, D_n)$, we say that n learner generalizes $G_n \\rightarrow_{n \\rightarrow \\infty} 0$\n",
    "            + In Practice, we never see $\\mathbb{P}$, tricks to approximate $R(h_n, \\mathbb{P})$\n",
    "                + Train-test split (when we have a lot of data)\n",
    "                + Cross Validation (when data is limited)\n",
    "                    + Leave one out\n",
    "                    + K fold\n",
    "                        + Split data into k sets $\\{D_1, \\cdots, D_k\\}$\n",
    "                        + For  $i =  1 , \\cdots , k$, train without $D_i$\n",
    "                        + Evaluate with $D_i$\n",
    "                        + Average performance over $k$ (why?)\n",
    "                            + Reduce the variance\n",
    "                        + Which one to choose?\n",
    "                            + Majority vote\n",
    "                            + Re-train on entire dataset\n",
    "                            + Pick one\n",
    "            + Not enough, since both performance can be awful\n",
    "                + Example : $h(x) = 1$ for all x, $x \\in [-B, B]$, $y = \\text{sign}$ then the training accuracy\n",
    "                $$\\text{acc}_{\\text{train}} = \\frac{1}{N}\\sum (1_{[y = 1]} + 1_{[y = 0]}) = \\frac{1}{N}\\sum 1_{[y = 1]}$$\n",
    "                and generalization accuracy\n",
    "                $$\\text{acc}(h_n, \\mathcal{P}) = \\frac{1}{2}$$\n",
    "                \n",
    "                Then generalization error gets small (Central Limit Theorem)\n",
    "    + Recall\n",
    "        + fraction of relevant instances that have been retrieved over the total amount of relevant instances\n",
    "        + $$Recall = \\frac{\\sum_{n = 1}^N 1_{[y = h(x) = 1]}}{\\sum_{n = 1}^N 1_{[y = 1]}}$$\n",
    "+ Utility $U(h, \\mathbb{P})$, the higher the better\n",
    "    + Accuracy $Acc(h, D) = 1 - $Error rate $= \\frac{1}{n} \\sum 1_{[y_i = h(x_i)]}$\n",
    "    + Precision\n",
    "\n",
    "### Design of Evaluation Metric\n",
    "+ Domain Knowledge \n",
    "+ Robustness\n",
    "+ Trade off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
