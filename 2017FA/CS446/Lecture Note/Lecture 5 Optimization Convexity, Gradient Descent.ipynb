{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting \n",
    "+ Maximum Likelihood\n",
    "    + Find parameters that \"most likely\"\n",
    "    $$\\hat{\\theta} = argmax_{\\theta} P(D_n \\mid \\theta)$$\n",
    "    $$\\hat{P} = P(x, y \\mid \\hat{\\theta})$$\n",
    "    usually use negative log likelihood\n",
    "    $$\\hat{\\theta} = argmin_{\\theta} -\\log P(D_n \\mid \\theta) =\\mathcal{L}(\\theta)$$\n",
    "    + Example 1 **Bernoulli**\n",
    "    $$P(x_i \\mid \\theta) = \\theta^{x_i}(1 - \\theta)^{1 - x_i}$$\n",
    "    \n",
    "    then, under iid model\n",
    "    \n",
    "    $$P(D_n \\mid \\theta) = \\prod_{x_i \\in D_n}\\theta^{x_i}(1 - \\theta)^{1 - x_i}$$\n",
    "    \n",
    "    so\n",
    "    \n",
    "    $$\\mathcal{L}(\\theta) = -\\sum_{x_i \\in D_n} P(x_i \\mid \\theta) = -\\sum_{x_i \\in D_n} x_i log \\theta + (1 - x_i) log (1 - \\theta)$$\n",
    "    \n",
    "    + $argmin_\\theta \\mathcal{L}(\\theta)$\n",
    "        + Solve $$\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta} = 0 \\Rightarrow \\theta = \\frac{1}{n} \\sum_{x_i \\in D_n} x_i$$\n",
    "        + More generally, use **gradient descent**\n",
    "            + Cons\n",
    "                + $$\\theta_{t + 1} = \\theta_t + \\alpha \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial \\theta}$$\n",
    "                + Only guarantee local optimality (initialization matters when non-convex)\n",
    "                + Can overshoot (step size matters)\n",
    "                + Can have trouble near saddle points\n",
    "            + Pros\n",
    "                + Can show convergence - Convex functions\n",
    "                + Pretty fast\n",
    "                + Bounded memory\n",
    "+ Maximum a posteriori\n",
    "+ Bayesian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q : Why do we use probablistic model\n",
    "+ Incorporating demain knowledge\n",
    "+ Model reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "\n",
    "## Recall\n",
    "\n",
    "## Generative Model\n",
    "\n",
    "Given $D_n$, find a good model $\\hat{P}$\n",
    "\n",
    "$P(x, y) \\approx \\hat{P}(x \\mid y)\\hat{P}(y)$\n",
    "\n",
    "**Naive Bayes** assumes \n",
    "$$p(x \\mid y) = \\prod_{i = 1}^d p(x_i \\mid y)$$\n",
    "\n",
    "Then by Bayesian Theorem $$p(y \\mid x) = \\frac{p(x \\mid y)p(y)}{p(x)}$$\n",
    "\n",
    "Need to know:\n",
    "+ $\\pi_c$ for $p(y = c \\mid \\pi_c) = \\text{Bernorlli}(\\pi_c)$ in which $\\pi_c = p(y = c)$, $c = \\{-1, 1\\}$ \n",
    "+ $\\{\\theta_j\\}^d_{j = 1}$\n",
    "\n",
    "Then we have $$P(x_j \\mid y, \\theta_j)$$\n",
    "\n",
    "## Example\n",
    "We have\n",
    "$$x_{ij} = \\{0, 1\\}$$\n",
    "$$P(x_{ij} \\mid y_i, \\theta_j) = \\theta^{x_{ij}}(1 - \\theta)^{1 - x_{ij}}$$\n",
    "\n",
    "then\n",
    "\\begin{align}\n",
    "    &\\mathcal{L}(\\theta) = -log(D_n \\mid \\theta) = \\sum_{k = 1}^c N_k log \\pi_k\\\\\n",
    "    &\\phantom{\\mathcal{L}(\\theta)} = -\\sum_{j = 1}^D \\sum_{k = 1}^C \\sum_{i : j_i = k} log P(x_{ij} \\mid \\theta_{jk})\n",
    "\\end{align}\n",
    "\n",
    "+ $$\\pi_k = \\frac{N_k}{N}$$\n",
    "+ $$N_k = \\sum_{i = 1}^n 1_{[y_i = k]}$$\n",
    "\n",
    "so\n",
    "\n",
    "+ $\\theta_{jk} = \\frac{N_{jk}}{N_k}$\n",
    "+ $N_{jk} = \\sum 1_{[x_{ij} = 1, y_i = k]}$\n",
    "\n",
    "## Prediction\n",
    "\n",
    "$$h_n = argmax_{f \\in \\mathcal{F}} ACC(h, \\mathbb{P})$$\n",
    "\n",
    "and $$ACC(h, \\mathbb{P}) = \\mathbb{E}[1_{[y_i = h_n(x_i)]}]$$\n",
    "\n",
    "We have Bayes Optimal\n",
    "$$P(y = k \\mid x) = \\frac{P(x \\mid y)P(y)}{P(x)} \\propto P(x \\mid y)P(y)$$\n",
    "\n",
    "and make prediction\n",
    "$$y = argmax_k P(y = k \\mid x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "We have \n",
    "$$\\mathcal{Y} = \\mathbb{R}$$\n",
    "$$R(h, \\mathbb{P}) = \\mathbb{E}[(y - h)^2]$$\n",
    "$$R(h, D_n) = \\mathbb{E}[(y_i - h(x_i))^2]$$\n",
    "\n",
    "In order to \n",
    "$$argmin_{f \\in \\mathcal{F}} E_p [y = f(x_i)]$$\n",
    "\n",
    "we have Bayes Optimal\n",
    "$$f^* = \\mathbb{E}_p[y \\mid x]$$\n",
    "\n",
    "## Model\n",
    "\n",
    "$$P(y \\mid x) = \\mathcal{N}(y \\mid w^Tx, \\sigma^2)$$\n",
    "\n",
    "### Naive bayes\n",
    "\n",
    "$$P(y \\mid x) = \\propto P(x \\mid y)P(y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
